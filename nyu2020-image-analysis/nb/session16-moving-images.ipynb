{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 16: Moving Images with the Distant Viewing Toolkit\n",
    "\n",
    "We further extend our techniques to working with moving images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "import urllib\n",
    "import keras\n",
    "import dvt\n",
    "\n",
    "import os\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DVT Demo\n",
    "\n",
    "We are going to look at a very short clip of an episode of Friends. Let's\n",
    "load in the functions that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dvt.annotate.core import FrameProcessor, FrameInput\n",
    "from dvt.annotate.diff import DiffAnnotator\n",
    "from dvt.annotate.face import FaceAnnotator, FaceDetectDlib, FaceEmbedVgg2\n",
    "from dvt.annotate.meta import MetaAnnotator\n",
    "from dvt.annotate.png import PngAnnotator\n",
    "from dvt.aggregate.cut import CutAggregator\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level='INFO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by constructing a frame input object attached to the video file. The bsize argument indicates that we will work with the video by looking through batches of 128 frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finput = FrameInput(join(\"..\", \"data\", \"video-clip.mp4\"), bsize=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create a frame processor and add four annotators: (i) metadata, (ii) png files, (iii) differences between successive frames, and (iv) faces. The quantiles input to the DiffAnnotator indicates that we want to compute the 40th percentile in differences between frames. The face detector take a long time to run when not on a GPU, so we restrict it to running only every 64 frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpobj = FrameProcessor()\n",
    "fpobj.load_annotator(PngAnnotator(output_dir=join(\"..\", \"video-clip-frames\")))\n",
    "fpobj.load_annotator(MetaAnnotator())\n",
    "fpobj.load_annotator(DiffAnnotator(quantiles=[40]))\n",
    "fpobj.load_annotator(FaceAnnotator(detector=FaceDetectDlib(), freq=64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run the pipeline of annotators over the input object. We will turn on logging here to see the output as Python processes each annotator over a batch of frames. The max_batch argument restricts the number of batches for testing purposes; set to None (default) to process the entire video file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpobj.process(finput, max_batch=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is now stored in the fpobj object. To access it, we call its collect_all method. This method returns a dictionary of custom objects (DictFrame, an extension of an ordered dictionary). Each can be converted to a Pandas data frame for ease of viewing the output or saving as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = fpobj.collect_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not look at each output type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata\n",
    "\n",
    "The metadata is not very exciting, but is useful for downstream tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj['meta'].todf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Png\n",
    "\n",
    "The png annotator does not return any data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj['png'].todf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, its used for its side-effects. You will see that there are individual frames from the video now saved in the directory \"video-clip-frames\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference\n",
    "\n",
    "The difference annotator indicates the differences between successive frames, as well as information about the average value (brightness) of each frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj['diff'].todf().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to find video cuts using these values? In order to aggregate the values into cuts, use the CutAggregator module. Here we have configured it to break a cut whenever the q40 key is at least 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cagg = CutAggregator(cut_vals={'q40': 3})\n",
    "cagg.aggregate(obj).todf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the constructed frames in \"video-clip-frames\", you should see that there are in fact breaks at frames 75 and 155."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face\n",
    "\n",
    "The face annotator detects faces in the frames. We configured it to only run every 64 frames, so there is only output in frames 0, 64, 128, and 192."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj['face'].todf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are two faces in frame 0, 64, and 192 but four faces detected in frame 128. In fact, all six of the main cast members are in frame 128, but two are two small and obscured to be found by the dlib algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
